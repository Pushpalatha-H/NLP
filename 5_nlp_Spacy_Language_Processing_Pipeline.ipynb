{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6TiRKM83bT0N0EiSp0qrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pushpalatha-H/NLP/blob/main/5_nlp_Spacy_Language_Processing_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## blank nlp\n",
        "\n",
        "[Text] --->[tokenizer]   [   ] ---->   [Doc]\n",
        "\n",
        "          --------nlp-------"
      ],
      "metadata": {
        "id": "HTssouC6Yc4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Text] ---> [tokenizer]  [pipeline]---> [Doc]\n",
        "\n",
        "          --------------nlp--------\n",
        "\n",
        "pipeline comes after tokenizer\n",
        "\n",
        "[Text] ---> [tokenizer] [tagger, parser, ner, ...] ---> [Doc]\n",
        "             \n",
        "            -----------nlp------------------------"
      ],
      "metadata": {
        "id": "19ZUVDFCXmCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "2BJPBamHYAKh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create blank nlp\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Captain america ate 100$ of samosa.Then he said I can do this all day.\")\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvjjIGETZkb3",
        "outputId": "72672bae-6a5d-4e83-db38-c802ea54231b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain\n",
            "america\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "samosa\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "I\n",
            "can\n",
            "do\n",
            "this\n",
            "all\n",
            "day\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load english language in sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "2kgSd-znbrK0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynw-ar_feJE2",
        "outputId": "aecbde91-5154-4c8e-94ca-1d85a64058ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rZK0HCGegh6",
        "outputId": "adc31df0-2094-414c-f174-d5235a5df316"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7bf437af6c20>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7bf437af6bc0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7bf4e95089e0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7bf437ae21c0>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7bf43785b680>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7bf4e95084a0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" Captain america ate 100$ of samosa.Then he said I can do this all day.\")\n",
        "for token in doc:\n",
        "  print(token, \"|\", token.pos_, \"|\", token.lemma_)           # .pos_ --> part of speach(noun, pronoun,..) , # .lemma_  --> base_word(ate is present give base word eat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJXl4j3mekqS",
        "outputId": "dae0b5b8-f22e-4c51-b18c-1f577b5c8e72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  | SPACE |  \n",
            "Captain | PROPN | Captain\n",
            "america | PROPN | america\n",
            "ate | VERB | eat\n",
            "100 | NUM | 100\n",
            "$ | NUM | $\n",
            "of | ADP | of\n",
            "samosa | NOUN | samosa\n",
            ". | PUNCT | .\n",
            "Then | ADV | then\n",
            "he | PRON | he\n",
            "said | VERB | say\n",
            "I | PRON | I\n",
            "can | AUX | can\n",
            "do | VERB | do\n",
            "this | PRON | this\n",
            "all | DET | all\n",
            "day | NOUN | day\n",
            ". | PUNCT | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf15wLxeRdLr",
        "outputId": "923d045a-22c2-4d14-c105-b5dfd8d1a13b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,\"|\" ,ent.label_, \"|\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jutsm2zNRn_q",
        "outputId": "1a906403-12bf-4194-85b4-f63137c3992e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
            "$45 billion | MONEY | Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying doc"
      ],
      "metadata": {
        "id": "jV9EYk53sS6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Bc6-1BnSsDrZ",
        "outputId": "29984435-70a4-47c3-b0ff-de84008041b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to acquire twitter for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show spacy\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrD9smA2cceQ",
        "outputId": "6ee494b7-3df8-48fe-a34a-0ea9bfec9a6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spacy\n",
            "Version: 3.7.4\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, smart-open, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
            "Required-by: en-core-web-sm, fastai\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French language\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "NC8ydoKQv7yB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,\"|\", ent.label_,\"|\",  spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGqN-3dKdqtq",
        "outputId": "d19e803a-c3b8-44b3-8c4b-555e966e7ac9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
            "Twitter | MISC | Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token,'|',  token.pos_,'|',  token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yAogeYReRhT",
        "outputId": "68b0824f-4988-4697-9593-e5a6356d24ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla | PROPN | Tesla\n",
            "Inc | PROPN | Inc\n",
            "va | VERB | aller\n",
            "racheter | VERB | racheter\n",
            "Twitter | VERB | twitter\n",
            "pour | ADP | pour\n",
            "$ | NOUN | dollar\n",
            "45 | NUM | 45\n",
            "milliards | NOUN | milliard\n",
            "de | ADP | de\n",
            "dollars | NOUN | dollar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "Uf2NZ1Y_fXRp",
        "outputId": "a0359ebb-b90e-4a86-b269-ebf2881d9478"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Language.pipe of <spacy.lang.fr.French object at 0x7bf512c8b0d0>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>spacy.language.Language.pipe</b><br/>def pipe(texts: Union[Iterable[Union[str, Doc]], Iterable[Tuple[Union[str, Doc], _AnyContext]]], *, as_tuples: bool=False, batch_size: Optional[int]=None, disable: Iterable[str]=SimpleFrozenList(), component_cfg: Optional[Dict[str, Dict[str, Any]]]=None, n_process: int=1) -&gt; Union[Iterator[Doc], Iterator[Tuple[Doc, _AnyContext]]]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/spacy/language.py</a>Process texts as a stream, and yield `Doc` objects in order.\n",
              "\n",
              "texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
              "    process.\n",
              "as_tuples (bool): If set to True, inputs should be a sequence of\n",
              "    (text, context) tuples. Output will then be a sequence of\n",
              "    (doc, context) tuples. Defaults to False.\n",
              "batch_size (Optional[int]): The number of texts to buffer.\n",
              "disable (List[str]): Names of the pipeline components to disable.\n",
              "component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
              "    arguments for specific components.\n",
              "n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
              "YIELDS (Doc): Documents in the order of the original text.\n",
              "\n",
              "DOCS: https://spacy.io/api/language#pipe</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1534);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8IeKgWj1dq",
        "outputId": "5ffd5f89-3a5d-49a0-88a7-27950bb49aa8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Capital america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.pos_, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8gcWNZTj4RJ",
        "outputId": "d33d651f-7992-4a3c-ff33-de9caa0b8c66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capital PROPN Capital\n",
            "america PROPN america\n",
            "ate VERB ate\n",
            "100 NUM 100\n",
            "$ NOUN dollar\n",
            "of ADP of\n",
            "samosa PRON samosa\n",
            ". PUNCT .\n",
            "Then NOUN then\n",
            "he ADP he\n",
            "said NOUN said\n",
            "I NOUN i\n",
            "can ADV can\n",
            "do ADP do\n",
            "this NOUN thi\n",
            "all NOUN all\n",
            "day ADJ day\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we only create pipeline by taking ner\n",
        "source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"ner\", source = source_nlp)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi2aA4qCmaK2",
        "outputId": "fbd1abbf-cd6e-4a1e-b658-908df6a669b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6F6CiUmtdPj",
        "outputId": "86de2f88-2e64-472d-f5cf-be3f1eb088a9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc ORG\n",
            "$45 billion MONEY\n"
          ]
        }
      ]
    }
  ]
}